{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Large Movie Review Dataset\n",
    "\n",
    "Hi again! You will be expected to finish this on your own, but you can use the available channels on Discord to ask questions and help others. Please read the entire notebook before starting, this will give you a better idea of what you need to accomplish.\n",
    "\n",
    "This project is related to NLP. As you may already know, the most important and hardest part of an NLP project is pre-processing, which is why we are going to focus on that.\n",
    "\n",
    "Regarding the data, we are not going to have a __csv file__, that would be too easy :) instead we are going to download the data from [AI Stanford Dataset](https://ai.stanford.edu/~amaas/data/sentiment/). When you download them you will notice that their format is text files, so you will have to work a little there to be able to use and process them. This is a dataset for __binary sentiment classification__.\n",
    "\n",
    "Basically a basic sentiment analysis problem, as in this case, consists of a classification problem, where the possible output labels are: `positive` and `negative`. Which indicates, if the review of a movie speaks positively or negatively. In our case it is a binary problem, but one could have many more \"feelings\" tagged and thus allow a more granular analysis.\n",
    "\n",
    "### These are the objectives of the project:\n",
    "\n",
    "* Read data that is not in a traditional format.\n",
    "* Put together a set of preprocessing functions that we can use later on any NLP or related problems.\n",
    "* Vectorize the data in order to apply a machine learning model to it: using BoW or TF-IDF.\n",
    "* BoW and TF-IDF are classic ways to vectorize text, but currently we have some more complex ways with better performance, for this we are going to train our own word embedding and use it as a vectorization source for our data.\n",
    "* Train a sentiment analysis model that allows us to detect positive and negative opinions in movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Get the data\n",
    "\n",
    "#### Download the data and process it in order to obtain:\n",
    "\n",
    "* `X_train:` list with reviews for training.\n",
    "* `y_train:` list with labels for training.\n",
    "* `X_test:` list with reviews for testing.\n",
    "* `y_test:` list with labels for testing.\n",
    "\n",
    "`Notes:` Use the target column as `positive`, that way the positive value will be indicated with a value of `1` and negative with a value of `0`. In this case, a split train/test is not necessary because the original data is already separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Don't change anything in this block, just make it run correctly*\n",
    "\n",
    "We are going to check that you have done it right and for that we are going to see if the dimensions match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(X_train, y_train, X_test, y_test):\n",
    "    if len(X_train) == len(y_train) == len(X_test) == len(y_test) == 25000:\n",
    "        print('Reading Data Success!')\n",
    "    else:\n",
    "        raise ValueError('Dimensions do not match!')\n",
    "\n",
    "check_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Normalize the data\n",
    "\n",
    "#### Create the following functions but not here in the notebook, do it in the python script called `text_normalizer.py` and import them into the notebook (this way you can build your own NLP preprocessing library). In fact, the structure of the functions is already written, you must complete them with the code that you consider necessary.\n",
    "\n",
    "#### Respect names and minimal interfaces:\n",
    "\n",
    "* `remove_html_tags(text):` to remove all HTML tags that may be present in text.\n",
    "* `remove_accented_chars(text):` to remove accented characters from text\n",
    "* `expand_contractions(text):` to expand contractions of the type, \"don't\" to \"do not\". The contractions are already defined in the \"contractions.py\" file.\n",
    "* `lemmatize_text(text):` to lemmatize text.\n",
    "* `stem_text(text):` to apply stemming (NLTK's PorterStemmer) on text.\n",
    "* `remove_special_chars(text):` to remove special characters from text.\n",
    "* `remove_special_chars(text, remove_digits=True):` to remove numbers, note that it is the same function to remove special characters with the addition of an argument that enables or disables the removal of numbers.\n",
    "* `remove_stopwords(text, stopwords=stop_words):` to remove stopwords from text.\n",
    "* `remove_extra_new_lines(text):` to remove extra newlines from text.\n",
    "* `remove_extra_whitespace(text):` to remove extra whitespaces from text.\n",
    "\n",
    "If you want to add more features that would be great, for example you could start by removing emojis, using different stemming algorithms, etc. The more functions you have the better, remember that the texts are very varied and the preprocessing depends a lot on the source of our data.\n",
    "\n",
    "To apply each of the functions you created and pre-process the dataset, you must use the `normalize_corpus` function of the `text_normalizer.py` script. In this method each of the functions you wrote is called, in fact you must enable or disable what you consider necessary (`at this point we leave it to your free choice, for example: you can lemmatize or apply stemming or directly not apply any of the two and so on with the rest, but that is your choice`), this function simply groups the previous ones for a more simplified use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Don't change anything in this block, just make it run correctly*\n",
    "\n",
    "We are going to check that the pre-processing does what we need it to do, for this we are going to test the functions with predefined inputs and we are going to see if the outputs match what we are looking for.\n",
    "\n",
    "Note that the functions are not defined in the notebook itself, but rather that they are in a python file and you must import them in order to use them. Same thing with names, you're going to have to name your functions the way they were named."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Inputs\n",
    "doc_html = \"\"\"\n",
    "<br /><br />But with plague out there and the news being kept a secret,\n",
    "the New Orleans PD starts a dragnet of the city's underworld.\n",
    "\"\"\"\n",
    "doc_accented = \"Héllo, thís is an accented sénténce.\"\n",
    "doc_contractions = \"I can't, because it doesn't work.\"\n",
    "doc_lemma = \"The striped bats are hanging on their feet for best\"\n",
    "doc_stem = \"\"\"\n",
    "Where did he learn to dance like that?\n",
    "His eyes were dancing with humor.\n",
    "She shook her head and danced away.\n",
    "\"\"\"\n",
    "doc_specials = \"hello? there A-Z-R_T(,**), world, welcome to python. this **should? the next line#followed- by@ an#other %million^ %%like $this.\"\n",
    "doc_digits = \"abc123def456ghi789zero0 hello my friend number 10\"\n",
    "doc_stop = \"He is a very good person\"\n",
    "doc_new_lines = \"\"\"we\n",
    "use\n",
    "a\n",
    "lot\n",
    "of\n",
    "lines\"\"\"\n",
    "doc_spaces = \"Hello           my      dear          friend\"\n",
    "\n",
    "# Outputs\n",
    "good_html = \"\"\"\n",
    "But with plague out there and the news being kept a secret,\n",
    "the New Orleans PD starts a dragnet of the city's underworld.\n",
    "\"\"\"\n",
    "good_accented = \"Hello, this is an accented sentence.\"\n",
    "good_contractions = \"I cannot, because it does not work.\"\n",
    "good_lemma = \"the stripe bat be hang on their foot for good\"\n",
    "good_stem = \"where did he learn to danc like that ? hi eye were danc with humor. she shook her head and danc away .\"\n",
    "good_specials = \"hello there AZRT world welcome to python this should the next linefollowed by another million like this\"\n",
    "good_digits = \"abcdefghizero hello my friend number \"\n",
    "good_stop = \"good person\"\n",
    "good_new_lines = \"we use a lot of lines\"\n",
    "good_spaces = \"Hello my dear friend\"\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def check_normalization():\n",
    "    if good_html == text_normalizer.remove_html_tags(doc_html):\n",
    "        print('[1/10] Remove HTML Success!')\n",
    "    else:\n",
    "        raise ValueError('[1/10] Remove HTML Fail!')\n",
    "        \n",
    "    if good_accented == text_normalizer.remove_accented_chars(doc_accented):\n",
    "        print('[2/10] Remove Accented Success!')\n",
    "    else:\n",
    "        raise ValueError('[2/10] Remove Accented Fail!')\n",
    "        \n",
    "    if good_contractions == text_normalizer.expand_contractions(doc_contractions):\n",
    "        print('[3/10] Expand Contractions Success!')\n",
    "    else:\n",
    "        raise ValueError('[3/10] Expand Contractions Fail!')\n",
    "        \n",
    "    if good_lemma == text_normalizer.lemmatize_text(doc_lemma):\n",
    "        print('[4/10] Lemmatization Success!')\n",
    "    else:\n",
    "        raise ValueError('[4/10] Lemmatization Fail!')\n",
    "        \n",
    "    print(text_normalizer.stem_text(doc_stem))\n",
    "    if good_stem == text_normalizer.stem_text(doc_stem):\n",
    "        print('[5/10] Stemming Success!')\n",
    "    else:\n",
    "        raise ValueError('[5/10] Stemming Fail!')\n",
    "        \n",
    "    if good_specials == text_normalizer.remove_special_chars(doc_specials):\n",
    "        print('[6/10] Remove Specials Success!')\n",
    "    else:\n",
    "        raise ValueError('[6/8] Remove Specials Fail!')\n",
    "        \n",
    "    if good_digits == text_normalizer.remove_special_chars(doc_digits, remove_digits=True):\n",
    "        print('[7/10] Remove Digits Success!')\n",
    "    else:\n",
    "        raise ValueError('[7/10] Remove Digits Fail!')\n",
    "        \n",
    "    if good_stop == text_normalizer.remove_stopwords(doc_stop, stopwords=stop_words):\n",
    "        print('[8/10] Remove Stopwords Success!')\n",
    "    else:\n",
    "        raise ValueError('[8/10] Remove Stopwords Fail!')\n",
    "    \n",
    "    if good_new_lines == text_normalizer.remove_extra_new_lines(doc_new_lines):\n",
    "        print('[9/10] Remove New Lines Success!')\n",
    "    else:\n",
    "        raise ValueError('[9/10] Remove New Lines Fail!')\n",
    "        \n",
    "    if good_spaces == text_normalizer.remove_extra_whitespace(doc_spaces):\n",
    "        print('[10/10] Remove Extra Whitespaces Success!')\n",
    "    else:\n",
    "        raise ValueError('[10/10] Remove Extra Whitespaces Fail!')\n",
    "\n",
    "check_normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering\n",
    "\n",
    "You already have the pr-eprocessed data, now you must vectorize them, because remember that the models only understand numbers. At this stage choose whether you want to vectorize with BoW or with TF-IDF. Later we will train our own embedding but for now we go with a more \"classic\" vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling and Performance Evaluation\n",
    "\n",
    "As we said at the beginning, what interests us most in this part is pre-processing. However, we must train a model, so choose a model of your choice (obviously a classification model, given the problem we are facing) and apply everything we learned. Also if you want you can try several models, the more models you use and know better!\n",
    "\n",
    "In addition to training the model we ask you to show:\n",
    "\n",
    "* `Precision`\n",
    "* `Recall`\n",
    "* `F1-Score`\n",
    "* `Classification Report`\n",
    "* `Confusion Matrix`\n",
    "\n",
    "To do this you must complete the `get_performance` function of the `evaluation.py` script.\n",
    "\n",
    "Also, you must complete the `plot_roc` function so that it can show:\n",
    "\n",
    "* `ROC Curve`\n",
    "* `Obtain the ROC-AUC value (later we will do a small minimum performance check with this value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Don't change anything in this block, just make it run correctly*\n",
    "\n",
    "Let's check that the `get_performance` function returns the metrics correctly. For that we are going to simulate input/output data of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_basic_metrics():\n",
    "    accuracy, precision, recall, f1_score = evaluation.get_performance(\n",
    "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0], \n",
    "        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "    )\n",
    "    \n",
    "    if (accuracy, precision, recall, f1_score) == (0.6, 0.6, 0.6, 0.6):\n",
    "        print('Success!')\n",
    "    else:\n",
    "        raise ValueError('You must check your get_performance function!')\n",
    "        \n",
    "check_basic_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if your model at least exceeds an ROC-AUC of 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_roc(roc_auc):\n",
    "    if roc_auc > 0.93:\n",
    "        print('Success!')\n",
    "    else:\n",
    "        raise ValueError('Your model is not good enough!')\n",
    "        \n",
    "check_roc(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Engineering with Custom Word Embedding\n",
    "\n",
    "### Tokenize reviews and train your own Word Embedding\n",
    "\n",
    "You are going to have to train your own word embedding, for this we are going to use the __gensim__ library. The only requirement we ask of you is that the $vector\\_size=100$.\n",
    "\n",
    "[Here](https://radimrehurek.com/gensim/models/word2vec.html) you can read Gensim's Word2Vec documentation so you can train your own embedding, using the review data as a corpus.\n",
    "\n",
    "As a previous step to training your word embedding you must tokenize the corpus, this may take a bit depending on the size of the dataset and the tokenizer we use, if you want you can try the NLTK tokenizer called `ToktokTokenizer`, which turns out to be a little faster (we hope that this recommendation does not bias your work, try and use the ones you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = None  # Replace with your trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate averaged word vector features\n",
    "\n",
    "Once the embedding has been trained, we must use it. Remember that embedding will convert each word you pass to it into a vector of a given dimension (in our case $vector\\_size=100$). So in order to obtain a vector for each review, you must average the vectors of all the words that are part of the same review.\n",
    "\n",
    "The function must have the following form:\n",
    "* `vectorizer(corpus, model, num_features=100)`\n",
    "\n",
    "\n",
    "Where:\n",
    "* `corpus:` corresponds to the entire dataset, in this way we obtain an average vector for each review, with a single call to the function.\n",
    "* `model:` is your trained model.\n",
    "* `num_features:` the dimension of the output vector of your embedding (remember that in our case we set this value to 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(corpus, model, num_features=100):\n",
    "    # Put your code\n",
    "    return corpus_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Don't change anything in this block, just make it run correctly*\n",
    "\n",
    "Let's do a simple check of the embedding that you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embedding(model):\n",
    "    vector = model.wv['computer']\n",
    "    if len(vector) == 100:\n",
    "        print(\n",
    "            'Success! Your embedding tells me that \"women\" and \"man\" '\n",
    "            f'are similar with a score of {model.wv.similarity(\"woman\", \"man\")}'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('You should check your embedding vector size!')\n",
    "        \n",
    "check_embedding(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to check the vectorizer, remember that the vectorizer must generate an average vector of all the words present in the same review. So we're going to get two vectors of two words and manually average them, then using those two words we'll simulate a tokenized sentence and see that it matches the manual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vectorizer(model):\n",
    "    vector1 = model.wv['personal']\n",
    "    vector2 = model.wv['computer']\n",
    "    avg = vectorizer([['personal', 'computer']], model)[0]\n",
    "\n",
    "    if np.allclose((vector1 + vector2) / 2, avg):\n",
    "        print('Success!')\n",
    "    else:\n",
    "        raise ValueError('You should check your vectorizer!')\n",
    "        \n",
    "check_vectorizer(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Finally train a new model, it can be the same one you used before and compare the results you got using BoW/TF-IDF and Word2Vec.\n",
    "\n",
    "In addition to training the model we ask you to show:\n",
    "\n",
    "* `Accuracy`\n",
    "* `Recall`\n",
    "* `F1-Score`\n",
    "* `Classification Report`\n",
    "* `Confusion Matrix`\n",
    "* `ROC Curve`\n",
    "* `Obtain the ROC-AUC value (later we will do a small minimum performance check with this value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Don't change anything in this block, just make it run correctly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_roc(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### OPTIONAL:\n",
    "\n",
    "In our case, we train a word embedding from scratch, which is very good at an educational level, but when applying it to a real problem, we need a lot of data (which is not the case with our problem). Therefore, we invite you to investigate and use one of the `pre-trained Word2Vec models`.\n",
    "\n",
    "If you look for the `Pretrained models` section in this [link](https://radimrehurek.com/gensim/models/word2vec.html), you will find information about the models that Gensim owns."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
